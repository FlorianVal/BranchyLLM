{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Branchy LLM Test\n",
    "\n",
    "This jupyter notebook is made to test out implementation of BranchyLLM on transformers library.\n",
    "\n",
    "## Testing Checklist for BranchyLLM Implementation\n",
    "\n",
    "- [x] **Load LLMs from Hugging Face using Transformers**\n",
    "  - Define a list of models to test\n",
    "  - Load each model\n",
    "  - Print initial model architecture and parameter count\n",
    "\n",
    "- [x] **Add Branches to Models**\n",
    "  - Implement branch insertion\n",
    "  - Display updated model architectures\n",
    "  - Compare parameter count (before vs. after)\n",
    "\n",
    "- [x] **Modify Number of Branches**\n",
    "  - Demonstrate dynamic branching\n",
    "  - Show updated architectures for different branch counts\n",
    "\n",
    "- [x] **Selective Branch Placement**\n",
    "  - Illustrate control over branch placement\n",
    "  - Test and display various configurations\n",
    "\n",
    "- [ ] **Inference with Early Exit**\n",
    "  - Perform inference demonstrating early exit\n",
    "  - Verify termination of calculations post-exit\n",
    "\n",
    "- [ ] **Train Branch Heads with Self-Supervision**\n",
    "  - Outline self-supervised training approach\n",
    "  - Implement training\n",
    "  - Display training results\n",
    "\n",
    "- [ ] **Computation Reduction Analysis**\n",
    "  - Define computation metrics\n",
    "  - Compare computation (before vs. after branching)\n",
    "\n",
    "- [ ] **Evaluate Model Performance**\n",
    "  - Define performance metrics\n",
    "  - Analyze performance degradation\n",
    "  - Perform comparative analysis across configurations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoModel, PreTrainedModel, AutoTokenizer\n",
    "from src.BranchyConfig import BranchyConfig\n",
    "from src.BranchyModel import BranchyModel\n",
    "from src.utils import print_model_parameter_distribution\n",
    "import torch\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load LLMs from Hugging Face using Transformers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4d9d0cdba34a48b5696f399a7f84ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhiForCausalLM(\n",
      "  (transformer): PhiModel(\n",
      "    (embd): Embedding(\n",
      "      (wte): Embedding(51200, 2560)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (h): ModuleList(\n",
      "      (0-31): 32 x ParallelBlock(\n",
      "        (ln): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (mixer): MHA(\n",
      "          (rotary_emb): RotaryEmbedding()\n",
      "          (Wqkv): Linear(in_features=2560, out_features=7680, bias=True)\n",
      "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          (inner_attn): SelfAttention(\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (inner_cross_attn): CrossAttention(\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): CausalLMHead(\n",
      "    (ln): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "    (linear): Linear(in_features=2560, out_features=51200, bias=True)\n",
      "  )\n",
      "  (loss): CausalLMLoss(\n",
      "    (loss_fct): CrossEntropyLoss()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define a list of allowed models\n",
    "allowed_models = [\n",
    "    \"microsoft/phi-2\",\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "]\n",
    "\n",
    "# Variable to select a model\n",
    "selected_model = allowed_models[0]  # Select the first model for demonstration\n",
    "\n",
    "# get config for base model\n",
    "model = AutoModelForCausalLM.from_pretrained(selected_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(selected_model)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.cache_utils import Cache\n",
    "from typing import Optional, List\n",
    "from torch import nn\n",
    "\n",
    "class BranchyModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    This class is a wrapper for transformer models with added functionality for branchy networks.\n",
    "    It uses BranchyConfig to initialize a model and later will be extended to add branches.\n",
    "\n",
    "    Args:\n",
    "        config (BranchyLLMConfig): The configuration to initialize the model with.\n",
    "        model (PreTrainedModel): The underlying transformer model to wrap.\n",
    "\n",
    "    Returns:\n",
    "        A model instance with the given configuration.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, model):\n",
    "        super().__init__(model.config)\n",
    "        # Initialize the base transformer model\n",
    "        self.model = model\n",
    "        \n",
    "        # Get args for branchy model\n",
    "        self.self_supervised_training = config.self_supervision\n",
    "        self.branch_locations = config.branch_locations\n",
    "        \n",
    "        # Get details on layering inside the model\n",
    "        if hasattr(self.model.config, \"n_layer\") or hasattr(self.model.config, \"num_hidden_layers\"): # If there is no n_layer in the config, there might be ways to get it from the model itself\n",
    "            self.num_layers = self.model.config.n_layer if hasattr(self.model.config, \"n_layer\") else self.model.config.num_hidden_layers\n",
    "            assert self.num_layers > 0, \"The number of layers must be greater than 0\"\n",
    "            assert len(self.branch_locations) < self.num_layers, \"The number of branches must be less than the number of layers\"\n",
    "            assert all([0 <= i < self.num_layers for i in self.branch_locations]), \"The branch locations must be between 0 and num_layers\"\n",
    "        else:\n",
    "            raise ValueError(\"cannot find n_layer in config\")\n",
    "            \n",
    "        # Make sure the base model is frozen\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Instantiate heads. Default: heads are copies of the lm_head\n",
    "        self.model.heads = torch.nn.ModuleList([copy.deepcopy(self.model.lm_head) for _ in range(len(self.branch_locations))])\n",
    "\n",
    "        # initialize heads\n",
    "        for head in self.model.heads:\n",
    "            head.apply(self.model._init_weights)\n",
    "            # Make them trainable\n",
    "            for param in head.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        self.post_init()\n",
    "    \n",
    "    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.prepare_inputs_for_generation \n",
    "    def prepare_inputs_for_generation(\n",
    "        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n",
    "    ):\n",
    "        print('used prepare_inputs_for_generation')\n",
    "        if past_key_values is not None:\n",
    "            if isinstance(past_key_values, Cache):\n",
    "                cache_length = past_key_values.get_seq_length()\n",
    "                past_length = past_key_values.seen_tokens\n",
    "                max_cache_length = past_key_values.get_max_length()\n",
    "            else:\n",
    "                cache_length = past_length = past_key_values[0][0].shape[2]\n",
    "                max_cache_length = None\n",
    "\n",
    "            # Keep only the unprocessed tokens:\n",
    "            # 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where\n",
    "            # some of the inputs are exclusivelly passed as part of the cache (e.g. when passing input_embeds as\n",
    "            # input)\n",
    "            if attention_mask is not None and attention_mask.shape[1] > input_ids.shape[1]:\n",
    "                input_ids = input_ids[:, -(attention_mask.shape[1] - past_length) :]\n",
    "            # 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard\n",
    "            # input_ids based on the past_length.\n",
    "            elif past_length < input_ids.shape[1]:  \n",
    "                input_ids = input_ids[:, past_length:]\n",
    "            # 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.\n",
    "\n",
    "            # If we are about to go beyond the maximum cache length, we need to crop the input attention mask.\n",
    "            if (\n",
    "                max_cache_length is not None\n",
    "                and attention_mask is not None\n",
    "                and cache_length + input_ids.shape[1] > max_cache_length\n",
    "            ):\n",
    "                attention_mask = attention_mask[:, -max_cache_length:]\n",
    "\n",
    "        position_ids = kwargs.get(\"position_ids\", None)\n",
    "        if attention_mask is not None and position_ids is None:\n",
    "            # create position_ids on the fly for batch generation\n",
    "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "            if past_key_values:\n",
    "                position_ids = position_ids[:, -input_ids.shape[1] :]\n",
    "\n",
    "        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n",
    "        if inputs_embeds is not None and past_key_values is None:\n",
    "            model_inputs = {\"inputs_embeds\": inputs_embeds}\n",
    "        else:\n",
    "            model_inputs = {\"input_ids\": input_ids}\n",
    "\n",
    "        model_inputs.update(\n",
    "            {\n",
    "                \"position_ids\": position_ids,\n",
    "                \"past_key_values\": past_key_values,\n",
    "                \"use_cache\": kwargs.get(\"use_cache\"),\n",
    "                \"attention_mask\": attention_mask,\n",
    "            }\n",
    "        )\n",
    "        return model_inputs\n",
    "    \n",
    "    def forward(self,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        *args,\n",
    "        **kwargs):\n",
    "        \n",
    "        if labels is not None:\n",
    "            return self.forward_for_training(labels=labels, *args, **kwargs)\n",
    "        else:\n",
    "            raise NotImplementedError(\"BranchyLlama is not yet implemented for inference\")\n",
    "        print('used forward')\n",
    "        return self.model(*args, **kwargs)\n",
    "    \n",
    "    def forward_for_training(self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = True,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        self_supervision: Optional[bool] = True):\n",
    "        \n",
    "        output_attentions = (\n",
    "            output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        )\n",
    "        return_dict = (\n",
    "            return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        )\n",
    "        if not output_hidden_states:\n",
    "            raise ValueError(\"output_hidden_states must be True for BranchyLlama\")\n",
    "        if self_supervision and labels is not None:\n",
    "            raise ValueError(\n",
    "                \"self_supervision and labels cannot be specified at the same time\"\n",
    "            )\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        hidden_states = outputs.hidden_states\n",
    "\n",
    "        # Compute logits for each head between each layer in the model\n",
    "        if self.branch_locations == []:\n",
    "            heads_logits = [\n",
    "                head(hidden_states[i].to(head.weight.dtype)).cpu() for i, head in enumerate(self.model.heads)\n",
    "            ]\n",
    "        # Only specific layers are branched\n",
    "        else:\n",
    "            heads_logits = []\n",
    "            for i, branch in enumerate(self.branch_locations):\n",
    "                heads_logits.append(self.model.heads[i](hidden_states[branch].to(self.model.heads[i].weight.dtype)).cpu())\n",
    "        lm_logits = self.lm_head(last_hidden_states).cpu()\n",
    "\n",
    "        heads_logits = torch.stack(heads_logits, dim=0).float()\n",
    "        lm_logits = lm_logits.float()\n",
    "        logits = torch.cat([heads_logits, lm_logits.unsqueeze(0)], dim=0)\n",
    "        # TODO finish here\n",
    "        loss = None\n",
    "        lm_loss = None\n",
    "        aux_loss = None\n",
    "\n",
    "        # Compute loss as in Llama implementation\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        lm_loss = self.compute_loss(lm_logits, labels, loss_fct)\n",
    "        aux_loss = self.compute_loss(heads_logits, labels, loss_fct)\n",
    "        loss = torch.stack([aux_loss, lm_loss], dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiForCausalLM(\n",
       "  (transformer): PhiModel(\n",
       "    (embd): Embedding(\n",
       "      (wte): Embedding(51200, 2560)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (h): ModuleList(\n",
       "      (0-31): 32 x ParallelBlock(\n",
       "        (ln): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (mixer): MHA(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (Wqkv): Linear(in_features=2560, out_features=7680, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (inner_attn): SelfAttention(\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (inner_cross_attn): CrossAttention(\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): CausalLMHead(\n",
       "    (ln): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "    (linear): Linear(in_features=2560, out_features=51200, bias=True)\n",
       "  )\n",
       "  (loss): CausalLMLoss(\n",
       "    (loss_fct): CrossEntropyLoss()\n",
       "  )\n",
       "  (heads): ModuleList(\n",
       "    (0-2): 3 x CausalLMHead(\n",
       "      (ln): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear): Linear(in_features=2560, out_features=51200, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# variables for branchy config\n",
    "branchy_config = BranchyConfig(self_supervision= True,\n",
    "                    num_branches= 3,\n",
    "                    branch_locations= [5, 10, 15])\n",
    "\n",
    "branchy_model = BranchyModel(branchy_config, model)\n",
    "branchy_model\n",
    "#print(branchy_model.model.heads)\n",
    "#print_model_parameter_distribution(branchy_model.model)\n",
    "\n",
    "# print parameters from lm_head and each heads to show they are not the same\n",
    "#print(next(branchy_model.model.lm_head.parameters())[0])\n",
    "#print(next(branchy_model.model.heads[0].parameters())[0])\n",
    "#print(next(branchy_model.model.heads[1].parameters())[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1212,  318,  281, 1672, 4226,   25,  220]])\n",
      "used forward\n",
      "['\\n']\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "prompt = \"This is an example script: \"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "print(inputs.input_ids) \n",
    "print(tokenizer.batch_decode(torch.argmax(branchy_model(inputs.input_ids).logits, dim=-1)[:,-1]))\n",
    "#generate_ids = branchy_model.generate(inputs.input_ids, max_length=20)\n",
    "#tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
